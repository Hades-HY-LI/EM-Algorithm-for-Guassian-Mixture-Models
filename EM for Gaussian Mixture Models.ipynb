{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "Hongye Li, Xiaochuan Ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Using cached scikit-image-0.19.2.tar.gz (22.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting imageio>=2.4.1\n",
      "  Using cached imageio-2.19.2-py3-none-any.whl (3.4 MB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/lihongye/miniforge3/envs/mltensor/lib/python3.8/site-packages (from scikit-image) (1.8.0)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.3.0-cp38-cp38-macosx_11_0_arm64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/lihongye/miniforge3/envs/mltensor/lib/python3.8/site-packages (from scikit-image) (9.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/lihongye/miniforge3/envs/mltensor/lib/python3.8/site-packages (from scikit-image) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lihongye/miniforge3/envs/mltensor/lib/python3.8/site-packages (from scikit-image) (21.3)\n",
      "Collecting networkx>=2.2\n",
      "  Using cached networkx-2.8.1-py3-none-any.whl (2.0 MB)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Using cached tifffile-2022.5.4-py3-none-any.whl (195 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lihongye/miniforge3/envs/mltensor/lib/python3.8/site-packages (from packaging>=20.0->scikit-image) (3.0.8)\n",
      "Building wheels for collected packages: scikit-image\n",
      "  Building wheel for scikit-image (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-image: filename=scikit_image-0.19.2-cp38-cp38-macosx_11_0_arm64.whl size=12173664 sha256=979b9b0eed657be90be4875e36e233213c1d4457e8e68c9fee32c5bf12903f37\n",
      "  Stored in directory: /Users/lihongye/Library/Caches/pip/wheels/ea/a1/bc/c81156493837d37793f73584eea3d414448636e50033d16627\n",
      "Successfully built scikit-image\n",
      "Installing collected packages: tifffile, PyWavelets, networkx, imageio, scikit-image\n",
      "Successfully installed PyWavelets-1.3.0 imageio-2.19.2 networkx-2.8.1 scikit-image-0.19.2 tifffile-2022.5.4\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/Users/lihongye/miniforge3/envs/mltensor/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import skimage.measure\n",
    "from skimage.util import view_as_blocks\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the data\n",
    "train_X=train_X/255\n",
    "test_X=test_X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of data with label 0-4\n",
    "train_ind=np.where(train_y<5)[0]\n",
    "test_ind=np.where(test_y<5)[0]\n",
    "ind_list=[]\n",
    "for i in range(5):\n",
    "    ind_list.append(np.where(train_y==i)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compress the image\n",
    "train_X_comp=skimage.measure.block_reduce(train_X, (1,2,2), np.mean)\n",
    "test_X_comp=skimage.measure.block_reduce(test_X, (1,2,2), np.mean)\n",
    "#image to vector\n",
    "train_X_comp=np.transpose(train_X_comp, (0, 2, 1))\n",
    "test_X_comp=np.transpose(test_X_comp, (0, 2, 1))\n",
    "train_X_vec=train_X_comp.reshape((train_X_comp.shape[0], 196))\n",
    "test_X_vec=test_X_comp.reshape((test_X_comp.shape[0], 196))\n",
    "#select the data with label 0-4\n",
    "train_X_vec_selected=train_X_vec[train_ind]\n",
    "train_y_selected=train_y[train_ind]\n",
    "test_X_vec_selected=test_X_vec[test_ind]\n",
    "test_y_selected=test_y[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5139, 196)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_vec_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture of spherical Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "The testing error is  0.11908931698774083\n",
      "The Log Likelyhood is  247738.3607195337\n"
     ]
    }
   ],
   "source": [
    "##initialization\n",
    "K=5\n",
    "N=train_X_vec_selected.shape[0]\n",
    "d=196\n",
    "mu=np.random.rand(K,d)\n",
    "sigma=np.random.rand(K,1)\n",
    "Pi=np.ones(K)/5\n",
    "ind_list1=[]\n",
    "for i in range(5):\n",
    "    ind_list1.append(np.where(train_y_selected==i)[0])\n",
    "##EM algorithm\n",
    "L_old=1\n",
    "Temp1=np.sum(train_X_vec_selected*train_X_vec_selected,axis=1,keepdims=True)\n",
    "for iter1 in range(100):\n",
    "    print(iter1)\n",
    "    #E step\n",
    "    M=(-0.5*(1/sigma)@Temp1.transpose()+(mu/sigma)@train_X_vec_selected.transpose()-0.5*d*np.log(2*math.pi*sigma)-0.5*(np.sum(mu*mu/sigma,axis=1,keepdims=True))).transpose()\n",
    "    Max_M=np.max(M, axis=1,keepdims=True)\n",
    "    F=M-Max_M\n",
    "    F=np.exp(F)\n",
    "    L=np.sum(F,axis=1,keepdims=True)\n",
    "    F=F/L\n",
    "    F+=0.00001*np.ones(F.shape)# add a small noise avoid 0 denorminator\n",
    "    F=F/np.sum(F,axis=1,keepdims=True)\n",
    "    #Log likelyhood\n",
    "    L=np.log(L)\n",
    "    L=np.sum(L)+np.sum(Max_M)\n",
    "    if abs(L_old-L)/abs(L)<0.0001:\n",
    "        break\n",
    "    L_old=L\n",
    "    #M_step\n",
    "    Sum_F = np.sum(F,axis=0,keepdims=True)\n",
    "    Pi=Sum_F/N\n",
    "    mu=F.transpose()@train_X_vec_selected\n",
    "    mu=mu/Sum_F.transpose()\n",
    "    Temp2 =-2*np.sum(F*(train_X_vec_selected@mu.transpose()),axis=0,keepdims=True)\n",
    "    Temp3=Sum_F*(np.sum(mu*mu,axis=1,keepdims=True)).transpose()\n",
    "    sigma = F.transpose()@Temp1+Temp2.transpose()+Temp3.transpose()\n",
    "    sigma = sigma/Sum_F.transpose()/d\n",
    "##Figure our how the groups of GMM correspond to true lable\n",
    "train_cluster_result=np.argmax(F,axis=1)\n",
    "G_to_L=[]\n",
    "for k1 in range(K):\n",
    "    temp=train_cluster_result[ind_list1[k1]]\n",
    "    current=0\n",
    "    relationship=[k1,k1]\n",
    "    for k2 in range(K):\n",
    "        if np.sum(temp==k2)>current:\n",
    "            current=np.sum(temp==k2)\n",
    "            relationship[1]=k2\n",
    "    G_to_L.append(relationship)    \n",
    "#testing\n",
    "N_test=test_X_vec_selected.shape[0]\n",
    "test_Temp1=np.sum(test_X_vec_selected*test_X_vec_selected,axis=1,keepdims=True)\n",
    "test_M=(-0.5*(1/sigma)@test_Temp1.transpose()+(mu/sigma)@test_X_vec_selected.transpose()-0.5*d*np.log(2*math.pi*sigma)-0.5*(np.sum(mu*mu/sigma,axis=1,keepdims=True))).transpose()\n",
    "test_Max_M=np.max(test_M, axis=1,keepdims=True)\n",
    "test_F=test_M-test_Max_M\n",
    "test_F=np.exp(test_F)\n",
    "test_L=np.sum(test_F,axis=1,keepdims=True)\n",
    "test_F=test_F/test_L\n",
    "test_L=np.log(test_L)\n",
    "test_L=np.sum(test_L)+np.sum(test_Max_M)\n",
    "test_result=np.argmax(test_F,axis=1)\n",
    "trans_test_y_selected=np.zeros([N_test])\n",
    "for i in range(N_test):\n",
    "    trans_test_y_selected[i]=G_to_L[test_y_selected[i]][1]\n",
    "print(\"The testing error is \", 1-np.sum(test_result==trans_test_y_selected)/N_test)\n",
    "print(\"The Log Likelyhood is \",test_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean error rate in 500 experiments is  0.1446904066939093\n",
      "The standard deviation of the error rate in 500 experiments is  0.05292906532868437\n"
     ]
    }
   ],
   "source": [
    "##500 experiments\n",
    "error_rate_list1=[]\n",
    "for _ in range(500):\n",
    "    ##initialization\n",
    "    K=5\n",
    "    N=train_X_vec_selected.shape[0]\n",
    "    d=196\n",
    "    mu=np.random.rand(K,d)\n",
    "    sigma=np.random.rand(K,1)\n",
    "    Pi=np.ones(K)/5\n",
    "    ind_list1=[]\n",
    "    for i in range(5):\n",
    "        ind_list1.append(np.where(train_y_selected==i)[0])\n",
    "    ##EM algorithm\n",
    "    L_old=1\n",
    "    Temp1=np.sum(train_X_vec_selected*train_X_vec_selected,axis=1,keepdims=True)\n",
    "    for iter1 in range(100):\n",
    "#         print(iter1)\n",
    "        #E step\n",
    "        M=(-0.5*(1/sigma)@Temp1.transpose()+(mu/sigma)@train_X_vec_selected.transpose()-0.5*d*np.log(2*math.pi*sigma)-0.5*(np.sum(mu*mu/sigma,axis=1,keepdims=True))).transpose()\n",
    "        Max_M=np.max(M, axis=1,keepdims=True)\n",
    "        F=M-Max_M\n",
    "        F=np.exp(F)\n",
    "        L=np.sum(F,axis=1,keepdims=True)\n",
    "        F=F/L\n",
    "        F+=0.00001*np.ones(F.shape)# add a small noise avoid 0 denorminator\n",
    "        F=F/np.sum(F,axis=1,keepdims=True)\n",
    "        #Log likelyhood\n",
    "        L=np.log(L)\n",
    "        L=np.sum(L)+np.sum(Max_M)\n",
    "        if abs(L_old-L)/abs(L)<0.0001:\n",
    "            break\n",
    "        L_old=L\n",
    "        #M_step\n",
    "        Sum_F = np.sum(F,axis=0,keepdims=True)\n",
    "        Pi=Sum_F/N\n",
    "        mu=F.transpose()@train_X_vec_selected\n",
    "        mu=mu/Sum_F.transpose()\n",
    "        Temp2 =-2*np.sum(F*(train_X_vec_selected@mu.transpose()),axis=0,keepdims=True)\n",
    "        Temp3=Sum_F*(np.sum(mu*mu,axis=1,keepdims=True)).transpose()\n",
    "        sigma = F.transpose()@Temp1+Temp2.transpose()+Temp3.transpose()\n",
    "        sigma = sigma/Sum_F.transpose()/d\n",
    "    ##Figure our how the groups of GMM correspond to true lable\n",
    "    train_cluster_result=np.argmax(F,axis=1)\n",
    "    G_to_L=[]\n",
    "    for k1 in range(K):\n",
    "        temp=train_cluster_result[ind_list1[k1]]\n",
    "        current=0\n",
    "        relationship=[k1,k1]\n",
    "        for k2 in range(K):\n",
    "            if np.sum(temp==k2)>current:\n",
    "                current=np.sum(temp==k2)\n",
    "                relationship[1]=k2\n",
    "        G_to_L.append(relationship)    \n",
    "    #testing\n",
    "    N_test=test_X_vec_selected.shape[0]\n",
    "    test_Temp1=np.sum(test_X_vec_selected*test_X_vec_selected,axis=1,keepdims=True)\n",
    "    test_M=(-0.5*(1/sigma)@test_Temp1.transpose()+(mu/sigma)@test_X_vec_selected.transpose()-0.5*d*np.log(2*math.pi*sigma)-0.5*(np.sum(mu*mu/sigma,axis=1,keepdims=True))).transpose()\n",
    "    test_Max_M=np.max(test_M, axis=1,keepdims=True)\n",
    "    test_F=test_M-test_Max_M\n",
    "    test_F=np.exp(test_F)\n",
    "    test_L=np.sum(test_F,axis=1,keepdims=True)\n",
    "    test_F=test_F/test_L\n",
    "    test_L=np.log(test_L)\n",
    "    test_L=np.sum(test_L)+np.sum(test_Max_M)\n",
    "    test_result=np.argmax(test_F,axis=1)\n",
    "    trans_test_y_selected=np.zeros([N_test])\n",
    "    for i in range(N_test):\n",
    "        trans_test_y_selected[i]=G_to_L[test_y_selected[i]][1]\n",
    "    error_rate_list1.append(1-np.sum(test_result==trans_test_y_selected)/N_test)\n",
    "print(\"The mean error rate in 500 experiments is \",np.mean(error_rate_list1))\n",
    "print(\"The standard deviation of the error rate in 500 experiments is \",np.std(error_rate_list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture of diagnal Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "The testing error is  0.11733800350262702\n",
      "The Log Likelyhood is  203869.15005684394\n"
     ]
    }
   ],
   "source": [
    "##initialization\n",
    "K=5\n",
    "N=train_X_vec_selected.shape[0]\n",
    "d=196\n",
    "mu=np.random.rand(K,d)\n",
    "sigma=np.random.rand(K,196)\n",
    "Pi=np.ones(K)/5\n",
    "ind_list2=[]\n",
    "for i in range(5):\n",
    "    ind_list2.append(np.where(train_y_selected==i)[0])\n",
    "##EM algorithm\n",
    "L_old=1\n",
    "Temp1=train_X_vec_selected*train_X_vec_selected\n",
    "for iter1 in range(100):\n",
    "    print(iter1)\n",
    "    #E step\n",
    "    M=(-0.5*(1/sigma)@Temp1.transpose()+(mu/sigma)@train_X_vec_selected.transpose()-0.5*d*np.log(2*math.pi)-0.5*(np.sum(mu*mu/sigma+np.log(sigma),axis=1,keepdims=True))).transpose()\n",
    "    Max_M=np.max(M, axis=1,keepdims=True)\n",
    "    F=M-Max_M\n",
    "    F=np.exp(F)\n",
    "    L=np.sum(F,axis=1,keepdims=True)\n",
    "    F=F/L\n",
    "    F+=0.00001*np.ones(F.shape)# add a small noise avoid 0 denorminator\n",
    "    F=F/np.sum(F,axis=1,keepdims=True)\n",
    "    #Log likelyhood\n",
    "    L=np.log(L)\n",
    "    L=np.sum(L)+np.sum(Max_M)\n",
    "    if abs(L_old-L)/abs(L)<0.0001:\n",
    "        break\n",
    "    L_old=L\n",
    "    #M_step\n",
    "    SUM_F = np.sum(F,axis=0,keepdims=True)\n",
    "    Pi=SUM_F/N\n",
    "    mu=F.transpose()@train_X_vec_selected\n",
    "    mu=mu/SUM_F.transpose()\n",
    "    Temp2=-2*mu*(F.transpose()@train_X_vec_selected)\n",
    "    Temp3=mu*mu*(F.transpose()@Temp1)\n",
    "    sigma = F.transpose()@Temp1+Temp2+Temp3\n",
    "    sigma = sigma/SUM_F.transpose()/d+0.05\n",
    "##Figure our how the groups of GMM correspond to true lable\n",
    "train_cluster_result=np.argmax(F,axis=1)\n",
    "G_to_L=[]\n",
    "for k1 in range(K):\n",
    "    temp=train_cluster_result[ind_list2[k1]]\n",
    "    current=0\n",
    "    relationship=[k1,k1]\n",
    "    for k2 in range(K):\n",
    "        if np.sum(temp==k2)>current:\n",
    "            current=np.sum(temp==k2)\n",
    "            relationship[1]=k2\n",
    "    G_to_L.append(relationship)    \n",
    "\n",
    "#testing\n",
    "N_test=test_X_vec_selected.shape[0]\n",
    "test_Temp1=test_X_vec_selected*test_X_vec_selected\n",
    "\n",
    "test_M=(-0.5*(1/sigma)@test_Temp1.transpose()+(mu/sigma)@test_X_vec_selected.transpose()-0.5*d*np.log(2*math.pi)-0.5*(np.sum(mu*mu/sigma+np.log(sigma),axis=1,keepdims=True))).transpose()\n",
    "test_Max_M=np.max(test_M, axis=1,keepdims=True)\n",
    "test_F=test_M-test_Max_M\n",
    "test_F=np.exp(test_F)\n",
    "test_L=np.sum(test_F,axis=1,keepdims=True)\n",
    "test_F=test_F/test_L\n",
    "test_L=np.log(test_L)\n",
    "test_L=np.sum(test_L)+np.sum(test_Max_M)\n",
    "test_result=np.argmax(test_F,axis=1)\n",
    "\n",
    "trans_test_y_selected=np.zeros([N_test])\n",
    "for i in range(N_test):\n",
    "    trans_test_y_selected[i]=G_to_L[test_y_selected[i]][1]\n",
    "print(\"The testing error is \", 1-np.sum(test_result==trans_test_y_selected)/N_test)\n",
    "print(\"The Log Likelyhood is \",test_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean error rate in 500 experiments is  0.1332387624051372\n",
      "The standard deviation of the error rate in 500 experiments is  0.04137207095309059\n"
     ]
    }
   ],
   "source": [
    "##500 experiments\n",
    "error_rate_list2=[]\n",
    "for xx in range(500):\n",
    "#     print(xx)\n",
    "    ##initialization\n",
    "    K=5\n",
    "    N=train_X_vec_selected.shape[0]\n",
    "    d=196\n",
    "    mu=np.random.rand(K,d)\n",
    "    sigma=np.random.rand(K,196)\n",
    "    Pi=np.ones(K)/5\n",
    "    ind_list2=[]\n",
    "    for i in range(5):\n",
    "        ind_list2.append(np.where(train_y_selected==i)[0])\n",
    "    ##EM algorithm\n",
    "    L_old=1\n",
    "    Temp1=train_X_vec_selected*train_X_vec_selected\n",
    "    for iter1 in range(100):\n",
    "#         print(iter1)\n",
    "        #E step\n",
    "\n",
    "        M=(-0.5*(1/sigma)@Temp1.transpose()+(mu/sigma)@train_X_vec_selected.transpose()-0.5*d*np.log(2*math.pi)-0.5*(np.sum(mu*mu/sigma+np.log(sigma),axis=1,keepdims=True))).transpose()\n",
    "        Max_M=np.max(M, axis=1,keepdims=True)\n",
    "        F=M-Max_M\n",
    "        F=np.exp(F)\n",
    "        L=np.sum(F,axis=1,keepdims=True)\n",
    "        F=F/L\n",
    "        F+=0.00001*np.ones(F.shape)# add a small noise avoid 0 denorminator\n",
    "        F=F/np.sum(F,axis=1,keepdims=True)\n",
    "        #Log likelyhood\n",
    "        L=np.log(L)\n",
    "        L=np.sum(L)+np.sum(Max_M)\n",
    "        if abs(L_old-L)/abs(L)<0.0001:\n",
    "            break\n",
    "        L_old=L\n",
    "        #M_step\n",
    "        SUM_F = np.sum(F,axis=0,keepdims=True)\n",
    "        Pi=SUM_F/N\n",
    "        mu=F.transpose()@train_X_vec_selected\n",
    "        mu=mu/SUM_F.transpose()\n",
    "        Temp2=-2*mu*(F.transpose()@train_X_vec_selected)\n",
    "        Temp3=mu*mu*(F.transpose()@Temp1)\n",
    "        sigma = F.transpose()@Temp1+Temp2+Temp3\n",
    "        sigma = sigma/SUM_F.transpose()/d+0.05\n",
    "    ##Figure our how the groups of GMM correspond to true lable\n",
    "    train_cluster_result=np.argmax(F,axis=1)\n",
    "    G_to_L=[]\n",
    "    for k1 in range(K):\n",
    "        temp=train_cluster_result[ind_list2[k1]]\n",
    "        current=0\n",
    "        relationship=[k1,k1]\n",
    "        for k2 in range(K):\n",
    "            if np.sum(temp==k2)>current:\n",
    "                current=np.sum(temp==k2)\n",
    "                relationship[1]=k2\n",
    "        G_to_L.append(relationship)    \n",
    "\n",
    "    #testing\n",
    "    N_test=test_X_vec_selected.shape[0]\n",
    "    test_Temp1=test_X_vec_selected*test_X_vec_selected\n",
    "\n",
    "    test_M=(-0.5*(1/sigma)@test_Temp1.transpose()+(mu/sigma)@test_X_vec_selected.transpose()-0.5*d*np.log(2*math.pi)-0.5*(np.sum(mu*mu/sigma+np.log(sigma),axis=1,keepdims=True))).transpose()\n",
    "    test_Max_M=np.max(test_M, axis=1,keepdims=True)\n",
    "    test_F=test_M-test_Max_M\n",
    "    test_F=np.exp(test_F)\n",
    "    test_L=np.sum(test_F,axis=1,keepdims=True)\n",
    "    test_F=test_F/test_L\n",
    "    test_L=np.log(test_L)\n",
    "    test_L=np.sum(test_L)+np.sum(test_Max_M)\n",
    "    test_result=np.argmax(test_F,axis=1)\n",
    "\n",
    "    trans_test_y_selected=np.zeros([N_test])\n",
    "    for i in range(N_test):\n",
    "        trans_test_y_selected[i]=G_to_L[test_y_selected[i]][1]\n",
    "    error_rate_list2.append(1-np.sum(test_result==trans_test_y_selected)/N_test)\n",
    "print(\"The mean error rate in 500 experiments is \",np.mean(error_rate_list2))\n",
    "print(\"The standard deviation of the error rate in 500 experiments is \",np.std(error_rate_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
